
environment

name: pli_env
channels:
  - milagraph
  - pyg
  - pytorch
  - anaconda
  - conda-forge
  - defaults
dependencies:
  - argon2-cffi=21.3.0
  - argon2-cffi-bindings=21.2.0
  - asttokens=2.0.5
  - attrs=21.4.0
  - autopep8=1.6.0
  - backcall=0.2.0
  - backports=1.0
  - backports.functools_lru_cache=1.6.4
  - black=21.12b0
  - blas=2.106
  - bleach=4.1.0
  - boost=1.74.0
  - boost-cpp=1.74.0
  - brotli=1.0.9
  - brotli-bin=1.0.9
  - brotlipy=0.7.0
  - bzip2=1.0.8
  - ca-certificates=2023.01.10
  - cairo=1.16.0
  - certifi=2022.12.7
  - cffi=1.15.0
  - charset-normalizer=2.0.10
  - click=8.0.3
  - colorama=0.4.4
  - cryptography=36.0.1
  - cudatoolkit=11.6.0
  - cycler=0.11.0
  - dataclasses=0.8
  - debugpy=1.5.1
  - decorator=5.1.1
  - defusedxml=0.7.1
  - entrypoints=0.3
  - executing=0.8.2
  - flit-core=3.6.0
  - fontconfig=2.13.1
  - fonttools=4.28.5
  - freetype=2.10.4
  - gettext=0.19.8.1
  - googledrivedownloader=0.4
  - greenlet=1.1.2
  - icu=68.2
  - idna=3.3
  - importlib-metadata=4.10.0
  - importlib_resources=5.4.0
  - intel-openmp=2022.0.0
  - ipykernel=6.7.0
  - ipython=8.0.0
  - ipython_genutils=0.2.0
  - ipywidgets=7.6.5
  - jbig=2.1
  - jedi=0.18.1
  - jinja2=3.0.3
  - joblib=1.1.0
  - jpeg=9d
  - jsonschema=4.4.0
  - jupyter_client=7.1.1
  - jupyter_core=4.9.1
  - jupyterlab_pygments=0.1.2
  - jupyterlab_widgets=1.0.2
  - kiwisolver=1.3.2
  - lcms2=2.12
  - lerc=3.0
  - libblas=3.9.0
  - libbrotlicommon=1.0.9
  - libbrotlidec=1.0.9
  - libbrotlienc=1.0.9
  - libcblas=3.9.0
  - libclang=11.1.0
  - libdeflate=1.8
  - libffi=3.4.2
  - libglib=2.70.2
  - libiconv=1.16
  - liblapack=3.9.0
  - liblapacke=3.9.0
  - libpng=1.6.37
  - libsodium=1.0.18
  - libtiff=4.3.0
  - libuv=1.43.0
  - libxml2=2.9.12
  - libzlib=1.2.11
  - lz4-c=1.9.3
  - m2w64-gcc-libgfortran=5.3.0
  - m2w64-gcc-libs=5.3.0
  - m2w64-gcc-libs-core=5.3.0
  - m2w64-gmp=6.1.0
  - m2w64-libwinpthread-git=5.0.0.4634.697f757
  - markupsafe=2.0.1
  - matplotlib=3.5.1
  - matplotlib-base=3.5.1
  - matplotlib-inline=0.1.3
  - mistune=0.8.4
  - mkl=2020.4
  - msys2-conda-epoch=20160418
  - munkres=1.1.4
  - mypy_extensions=0.4.3
  - nbclient=0.5.10
  - nbconvert=6.4.0
  - nbformat=5.1.3
  - nest-asyncio=1.5.4
  - networkx=2.6.3
  - ninja=1.10.2
  - notebook=6.4.7
  - numpy=1.22.0
  - olefile=0.46
  - openjpeg=2.4.0
  - openssl=1.1.1t
  - packaging=21.3
  - pandas=1.3.5
  - pandoc=2.17
  - pandocfilters=1.5.0
  - parso=0.8.3
  - pathlib=1.0.1
  - pathspec=0.9.0
  - patsy=0.5.3
  - pcre=8.45
  - pickleshare=0.7.5
  - pillow=8.4.0
  - pip=21.2.2
  - pixman=0.40.0
  - platformdirs=2.3.0
  - prometheus_client=0.12.0
  - prompt-toolkit=3.0.24
  - pure_eval=0.2.1
  - pycairo=1.20.1
  - pycodestyle=2.10.0
  - pycparser=2.21
  - pygments=2.11.2
  - pyopenssl=21.0.0
  - pyparsing=3.0.6
  - pyqt=5.12.3
  - pyqt-impl=5.12.3
  - pyqt5-sip=4.19.18
  - pyqtchart=5.12
  - pyqtwebengine=5.12.1
  - pyrsistent=0.18.0
  - pysocks=1.7.1
  - python=3.8.12
  - python-dateutil=2.8.2
  - python-louvain=0.15
  - python_abi=3.8
  - pytorch=1.12.1=py3.8_cuda11.6_cudnn8_0
  - pytorch-mutex=1.0=cuda
  - pytorch-scatter=2.0.9=py38_torch_1.12.0_cu116
  - pytz=2021.3
  - pywin32=303
  - pywinpty=0.5.7
  - pyyaml=6.0
  - pyzmq=22.3.0
  - qt=5.12.9
  - rdkit=2021.09.4
  - reportlab=3.5.68
  - requests=2.27.1
  - runipy=0.1.5
  - scikit-learn=1.0.2
  - scipy=1.7.3
  - seaborn=0.12.1
  - seaborn-base=0.12.1
  - send2trash=1.8.0
  - setuptools=58.0.4
  - six=1.16.0
  - sqlalchemy=1.4.29
  - sqlite=3.37.0
  - stack_data=0.1.4
  - statsmodels=0.13.2
  - terminado=0.9.4
  - testpath=0.5.0
  - threadpoolctl=3.0.0
  - tk=8.6.11
  - toml=0.10.2
  - tomli=1.2.2
  - torchdrug=0.1.2
  - tornado=6.1
  - tqdm=4.62.3
  - traitlets=5.1.1
  - typed-ast=1.5.1
  - typing_extensions=4.0.1
  - unicodedata2=14.0.0
  - urllib3=1.26.8
  - vc=14.2
  - vs2015_runtime=14.27.29016
  - wcwidth=0.2.5
  - webencodings=0.5.1
  - wheel=0.37.1
  - widgetsnbextension=3.5.2
  - win_inet_pton=1.1.0
  - wincertstore=0.2
  - winpty=0.4.3
  - xz=5.2.5
  - yacs=0.1.6
  - yaml=0.2.5
  - zeromq=4.3.4
  - zipp=3.7.0
  - zlib=1.2.11
  - zstd=1.5.1
  - pip:
    - pbr==5.8.0
    - pysmiles==1.0.1
    - torch-cluster==1.6.0
    - torch-geometric==2.1.0.post1
    - torch-sparse==0.6.15
    - torch-spline-conv==1.2.1
prefix: YOUR/CONDA_ENVS_PATH/pli_env



trainer:
  DATA_PATH: "data/" 
  SAVE_BEST_MODEL: True
  MODEL_SAVE_FOLDER: "models" 
  HIDDEN_CHANNELS: 256
  BATCH_SIZE: 32
  EPOCHS: 100
  SEED: 42
  CLEAN_DATA: True
  MIN_AFFINITY: 5
  MAX_AFFINITY: 11
  MEAN_LOWER_BOUND: 6.5
  MEAN_UPPER_BOUND: 7.5
  LOW_BOUND: 6
  HIGH_BOUND: 8
  GNN_MODEL: "GCN" # "GC_GNN" "GCN" "GAT" "GIN" "GINE" "GraphSAGE"
  EDGE_WEIGHT: True
  SCALING: True
  SEED: 42
  HIDDEN_CHANNELS: 256
  EPOCHS: 100
  LEARNING_RATE: 1e-3
  WEIGHT_DECAY: 5e-4
  NODE_FEATURES: True

explainer:
  DATA_PATH: "data/"
  SAVE_FOLDER: 'results/explanations/'
  MODEL_PATH: 'models/pretrained_models/model_GCN_best_92.ckpt' 
  GNN_MODEL: "GCN" # "GC_GNN" "GCN" "GAT" "GIN" "GINE" "GraphSAGE"
  SAMPLES_TO_EXPLAIN: 20 #number of test samples to explain
  AFFINITY_SET: "low" 
  HIDDEN_CHANNELS: 256
  BATCH_SIZE: 32
  EPOCHS: 100
  SEED: 42
  CLEAN_DATA: True
  MIN_AFFINITY: 5
  MAX_AFFINITY: 11
  MEAN_LOWER_BOUND: 6.5
  MEAN_UPPER_BOUND: 7.5
  LOW_BOUND: 6
  HIGH_BOUND: 8
  EDGE_WEIGHT: True 
  SCALING: True
  SEED: 42
  HIDDEN_CHANNELS: 256
  EPOCHS: 100
  LEARNING_RATE: 1e-3
  WEIGHT_DECAY: 5e-4
  NODE_FEATURES: True

top_k_computation:
  DATA_PATH: "data/" 
  PLOT: True 
  EXPLANATIONS_FOLDER: "results/explanations/GraphSAGE/" 
  TOP_K_VALUES: [5,10,15,20,25] 
  NODE_LABELS: False 
  IMAGE_FORMAT: "svg" 





statistics:
  DATA_PATH: "data/" 
  EXPLANATIONS_FOLDER: "results/explanations/GC_GNN/" 
  TOP_K_VALUES: [25] 
  MIN_AFFINITY: 5
  MAX_AFFINITY: 11
  MEAN_LOWER_BOUND: 6.5
  MEAN_UPPER_BOUND: 7.5
  LOW_BOUND: 6
  HIGH_BOUND: 8
  CLEAN_DATA: True

affinity_shifting:
  DATA_PATH: "data/" 
  MODEL_PATH: 'models/pretrained_models/model_GAT_best_98.ckpt' 
  GNN_MODEL: "GAT" # "GC_GNN" "GCN" "GAT" "GIN" "GINE" "GraphSAGE"
  HIDDEN_CHANNELS: 256
  BATCH_SIZE: 32
  EPOCHS: 100
  SEED: 42
  CLEAN_DATA: True
  MIN_AFFINITY: 5
  MAX_AFFINITY: 11
  MEAN_LOWER_BOUND: 6.5
  MEAN_UPPER_BOUND: 7.5
  LOW_BOUND: 6
  HIGH_BOUND: 8
  EDGE_WEIGHT: True
  SCALING: True
  SEED: 42
  HIDDEN_CHANNELS: 256
  EPOCHS: 100
  LEARNING_RATE: 1e-3
  WEIGHT_DECAY: 5e-4
  NODE_FEATURES: True
  AFFINITY_SHIFT: 0.5

gnn_explainer:
  DATA_PATH: "data/" 
  SAVE_FOLDER: 'results/additional_experiments/gnn_explainer_explanations/' 
  MODEL_PATH: 'models/pretrained_models/model_GraphSAGE_best_71.ckpt' 
  GNN_MODEL: "GraphSAGE" 
  SAMPLES_TO_EXPLAIN: 20 
  AFFINITY_SET: "low" 
  HIDDEN_CHANNELS: 256
  BATCH_SIZE: 32
  EPOCHS: 100
  SEED: 42
  CLEAN_DATA: True
  MIN_AFFINITY: 5
  MAX_AFFINITY: 11
  MEAN_LOWER_BOUND: 6.5
  MEAN_UPPER_BOUND: 7.5
  LOW_BOUND: 6
  HIGH_BOUND: 8
  EDGE_WEIGHT: True #NOTE: If set to True, this has no effect with GraphSAGE and GIN since they do not support edge weights
  SCALING: True
  SEED: 42
  HIDDEN_CHANNELS: 256
  EPOCHS: 100
  LEARNING_RATE: 1e-3
  WEIGHT_DECAY: 5e-4
  NODE_FEATURES: True  




import os
from tqdm import tqdm
import json
import yaml

import torch
from torch_geometric.data import Data

import numpy as np
import matplotlib.pyplot as plt

import networkx as nx


from src.utils import create_edge_index, PLIDataset



with open("parameters.yml") as paramFile:  
        args = yaml.load(paramFile, Loader=yaml.FullLoader)

DATA_PATH = args["top_k_computation"]["DATA_PATH"]

PLOT = args["top_k_computation"]["PLOT"]
EXPLANATIONS_FOLDER = args["top_k_computation"]["EXPLANATIONS_FOLDER"]

TOP_K_VALUES = args["top_k_computation"]["TOP_K_VALUES"]
NODE_LABELS = args["top_k_computation"]["NODE_LABELS"]
IMAGE_FORMAT = args["top_k_computation"]["IMAGE_FORMAT"]

AFFINITY_GROUPS = ["low affinity", "medium affinity", "high affinity"]

#reduced version of the dataset builder to save computation time and memory
def generate_pli_dataset_dict_reduced(data_path):

    directory = os.fsencode(data_path)

    dataset_dict = {}
    dirs = os.listdir(directory)
    for file in tqdm(dirs):
        interaction_name = os.fsdecode(file)

        
        if os.path.isdir(data_path + interaction_name):
            dataset_dict[interaction_name] = {}
            G = None
            with open(data_path + interaction_name + "/" + interaction_name + "_interaction_graph.json", 'r') as f:
                data = json.load(f)
                G = nx.Graph()

                for node in data['nodes']:
                    G.add_node(node["id"], atom_type=node["attype"], origin=node["pl"]) 

                for edge in data['edges']:
                    if edge["id1"] != None and edge["id2"] != None:
                        G.add_edge(edge["id1"], edge["id2"], weight= float(edge["length"]))
                        

                for node in data['nodes']:
                    nx.set_node_attributes(G, {node["id"]: node["attype"]}, "atom_type")
                    nx.set_node_attributes(G, {node["id"]: node["pl"]}, "origin")

                
            dataset_dict[interaction_name]["networkx_graph"] = G
            edge_index, edge_weight = create_edge_index(G, weighted=True)

            dataset_dict[interaction_name]["edge_index"] = edge_index
            dataset_dict[interaction_name]["edge_weight"] = edge_weight
            

            num_nodes = G.number_of_nodes()
            
            dataset_dict[interaction_name]["x"] = torch.full((num_nodes, 1), 1.0, dtype=torch.float) #dummy feature

    
    return dataset_dict
        
pli_dataset_dict = generate_pli_dataset_dict_reduced(DATA_PATH + "/dataset/")

data_list = []
for interaction_name in tqdm(pli_dataset_dict):
    edge_weight_sample = None
    edge_weight_sample = pli_dataset_dict[interaction_name]["edge_weight"]
    data_list.append(Data(x = pli_dataset_dict[interaction_name]["x"], edge_index = pli_dataset_dict[interaction_name]["edge_index"], edge_weight = pli_dataset_dict[interaction_name]["edge_weight"], networkx_graph = pli_dataset_dict[interaction_name]["networkx_graph"], interaction_name = interaction_name))

dataset = PLIDataset(".", data_list = data_list)

train_interactions = []
val_interactions = []
core_set_interactions = []
hold_out_interactions = []

with open(DATA_PATH + "pdb_ids/training_set.csv", 'r') as f:
    train_interactions = f.readlines()

train_interactions = [interaction.strip() for interaction in train_interactions]

with open(DATA_PATH + "pdb_ids/validation_set.csv", 'r') as f:
    val_interactions = f.readlines()

val_interactions = [interaction.strip() for interaction in val_interactions]

with open(DATA_PATH + "pdb_ids/core_set.csv", 'r') as f:
    core_set_interactions = f.readlines()

core_set_interactions = [interaction.strip() for interaction in core_set_interactions]

with open(DATA_PATH + "pdb_ids/hold_out_set.csv", 'r') as f:
    hold_out_interactions = f.readlines()

hold_out_interactions = [interaction.strip() for interaction in hold_out_interactions]

train_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in train_interactions]
val_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in val_interactions]
core_set_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in core_set_interactions]
hold_out_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in hold_out_interactions]

for affinity_group in AFFINITY_GROUPS:
    
    print("Computing top k for " + affinity_group + " set")

    num_relevant_edge_in_protein_list = {key: [] for key in TOP_K_VALUES}
    num_relevant_edge_in_ligand_list = {key: [] for key in TOP_K_VALUES}
    num_relevant_edge_in_between_list = {key: [] for key in TOP_K_VALUES}
    num_total_relevant_edges_list = {key: [] for key in TOP_K_VALUES}

    num_relevant_absolute_edge_in_protein_list = {key: [] for key in TOP_K_VALUES}
    num_relevant_absolute_edge_in_ligand_list = {key: [] for key in TOP_K_VALUES}
    num_relevant_absolute_edge_in_between_list = {key: [] for key in TOP_K_VALUES}
    num_total_relevant_absolute_list = {key: [] for key in TOP_K_VALUES}

    num_total_edge_in_protein_list = []
    num_total_edge_in_ligand_list = []
    num_total_edge_in_between_list = []
    num_total_edges_in_graph_list = []

    directory = EXPLANATIONS_FOLDER + affinity_group + "/"
    test_interaction_name = None
    test_interaction_index = None

   

    for file in tqdm(os.listdir(directory)):
        test_interaction_name = os.fsdecode(file)
        # if test_interaction_name != SELECTED_INTERACTION_NAME:
        #     continue

        test_interaction_path = directory + test_interaction_name
        if os.path.isdir(test_interaction_path):
            for i, interaction in enumerate(hold_out_data):
                if interaction.interaction_name == test_interaction_name:
                    test_interaction_index = i
                    break
        else:
            continue
        
        

        test_interaction = hold_out_data[test_interaction_index]

        #read phi_edges from file
        phi_edges = []
        

        with open(directory + test_interaction_name + "/" + test_interaction_name + "_statistics.txt", 'r') as f:
            
            shapley_computed = False
            while not shapley_computed:
                line = f.readline()
                if line.strip().startswith("Shapley") or line.strip().startswith("Attributions"):
                    f.readline()
                    lines = f.readlines()
                    for line in lines:
                        phi_edges.append(float(line.strip().split(" ")[-1]))
                    shapley_computed = True

        
        #plotting
        num_bonds = test_interaction.networkx_graph.number_of_edges()


        rdkit_bonds_phi = [0]*num_bonds
        rdkit_bonds = {}

        bonds = dict(test_interaction.networkx_graph.edges())
        bonds = list(bonds.keys())

        for i in range(num_bonds):
            init_atom = bonds[i][0]
            end_atom = bonds[i][1]
            
            rdkit_bonds[(init_atom, end_atom)] = i

        
        
        for i in range(len(phi_edges)):
            phi_value = phi_edges[i]
            init_atom = test_interaction.edge_index[0][i].item()
            end_atom = test_interaction.edge_index[1][i].item()
            
            if (init_atom, end_atom) in rdkit_bonds:
                bond_index = rdkit_bonds[(init_atom, end_atom)]
                rdkit_bonds_phi[bond_index] += phi_value
            if (end_atom, init_atom) in rdkit_bonds:
                bond_index = rdkit_bonds[(end_atom, init_atom)]
                rdkit_bonds_phi[bond_index] += phi_value
                


        G = test_interaction.networkx_graph
        colors = ["red" if G.nodes[node]["origin"] == "L" else "lightblue" for node in G.nodes]

        num_total_edge_in_protein = 0
        num_total_edge_in_ligand = 0
        num_total_edge_in_between = 0

        atoms_origin = nx.get_node_attributes(G, 'origin')

        for bond in bonds:
            init_atom = bond[0]
            end_atom = bond[1]

            if atoms_origin[init_atom] == "P" and atoms_origin[end_atom] == "P":
                num_total_edge_in_protein += 1
            elif atoms_origin[init_atom] == "L" and atoms_origin[end_atom] == "L":
                num_total_edge_in_ligand += 1
            else:
                num_total_edge_in_between += 1

        num_total_edges_in_graph = num_total_edge_in_protein + num_total_edge_in_ligand + num_total_edge_in_between

        num_total_edge_in_protein_list.append(num_total_edge_in_protein)
        num_total_edge_in_ligand_list.append(num_total_edge_in_ligand)
        num_total_edge_in_between_list.append(num_total_edge_in_between)
        num_total_edges_in_graph_list.append(num_total_edges_in_graph)


        
        with open(directory + test_interaction_name + "/" + test_interaction.interaction_name + "_statistics_top_k_edges.txt", "w+") as f:
            f.write("Top k edges statistics\n\n")

        absolute_phi = np.abs(rdkit_bonds_phi)
        #sort indices according to decreasing phi values
        indices_sorted = np.argsort(-absolute_phi)
        
        for top_k_t in TOP_K_VALUES:

            top_edges = indices_sorted[:top_k_t]

            num_total_top_abs_edges = top_k_t
            num_edge_in_protein = 0
            num_edge_in_ligand = 0
            num_edge_in_between = 0

            atoms_origin = nx.get_node_attributes(G, 'origin')

            edges_to_draw = []
            edges_colors = []
            edges_widths = []
            for bond in bonds:
                init_atom = bond[0]
                end_atom = bond[1]

                bond_index = rdkit_bonds[(init_atom, end_atom)]
                if bond_index in top_edges:
                    if atoms_origin[init_atom] == "P" and atoms_origin[end_atom] == "P":
                        num_edge_in_protein += 1
                        edges_colors.append("darkblue")
                    elif atoms_origin[init_atom] == "L" and atoms_origin[end_atom] == "L":
                        num_edge_in_ligand += 1
                        edges_colors.append("darkred")
                    else:
                        num_edge_in_between += 1
                        edges_colors.append("darkgreen")
                    edges_widths.append(3)
                
                    edges_to_draw.append((init_atom, end_atom))
                else:
                    edges_colors.append("lightgrey") 
                    edges_widths.append(1.5)

            if PLOT:

                #draw graph with important edges
                plt.figure(figsize=(10,10))
                pos = nx.spring_layout(G)

                nx.draw(G, pos=pos, node_size = 400, with_labels=NODE_LABELS, font_weight='bold', labels=nx.get_node_attributes(G, 'atom_type'), node_color=colors,edge_color=edges_colors, width=edges_widths, edge_cmap=plt.cm.bwr)   

                plt.savefig(directory + test_interaction_name + "/" + test_interaction.interaction_name + "_EdgeSHAPer_top_" + str(top_k_t) + "_edges_full_graph." + IMAGE_FORMAT, dpi=300)
                
                plt.close()

                #save original graph
                if top_k_t == 25:
                    plt.figure(figsize=(10,10))
                    
                    nx.draw(G, pos=pos, with_labels=NODE_LABELS, font_weight='bold', labels=nx.get_node_attributes(G, 'atom_type'), node_color=colors)
                    
                    plt.savefig(directory + test_interaction_name + "/" + test_interaction.interaction_name + "_full_interaction_graph." + IMAGE_FORMAT, dpi=300)
                    
                    plt.close()
                
            
            with open(directory + test_interaction_name + "/" + test_interaction.interaction_name + "_statistics_top_k_edges.txt", "a") as f:
                f.write("Top " + str(top_k_t) + " relevant edges\n\n")
                if num_total_edge_in_protein == 0:
                    f.write("Number of relevant edges connecting protein pseudo-atoms: 0\n")
                    
                else:
                    f.write("Number of relevant edges connecting protein pseudo-atoms: " + str(num_edge_in_protein) + "\n")
                    f.write("% w.r.t. total number of relevant edges: " + str(round((num_edge_in_protein/num_total_top_abs_edges)*100, 1)) + "%\n")
                    
                if num_total_edge_in_ligand == 0:
                    f.write("Number of relevant edges connecting ligand pseudo-atoms: 0\n")
                    
                else:
                    f.write("Number of relevant edges connecting ligand pseudo-atoms: " + str(num_edge_in_ligand) + "\n")
                    f.write("% w.r.t. total number of relevant edges: " + str(round((num_edge_in_ligand/num_total_top_abs_edges)*100, 1)) + "%\n")
                    
                if num_total_edge_in_between == 0:
                    f.write("Number of relevant edges connecting protein and ligand pseudo-atoms: 0\n")
                    
                else:
                    f.write("Number of relevant edges connecting protein and ligand pseudo-atoms: " + str(num_edge_in_between) + "\n")
                    f.write("% w.r.t. total number of relevant edges: " + str(round((num_edge_in_between/num_total_top_abs_edges)*100, 1)) + "%\n\n")
                    
                num_total_top_abs_edges = num_edge_in_protein + num_edge_in_ligand + num_edge_in_between
                num_relevant_edge_in_protein_list[top_k_t].append(num_edge_in_protein)
                num_relevant_edge_in_ligand_list[top_k_t].append(num_edge_in_ligand)
                num_relevant_edge_in_between_list[top_k_t].append(num_edge_in_between)
                num_total_relevant_edges_list[top_k_t].append(num_total_top_abs_edges)
                # print("num_edge_in_protein: " + str(num_total_top_abs_edges))
    
    with open(directory + "/statistics_top_k_edges.txt", "w+") as f:
        f.write("Top k edges statistics\n\n")

    for top_k_t in TOP_K_VALUES:

        with open(directory + "/statistics_top_k_edges.txt", "a") as f:

            f.write("Top " + str(top_k_t) + " relevant edges\n\n")
           
            f.write("Avg number of relevant edges in protein: " +  str(round(np.mean(num_relevant_edge_in_protein_list[top_k_t]), 3)) + "\n")
            f.write("% w.r.t. total number of relevant edges: " + str(round((np.mean(num_relevant_edge_in_protein_list[top_k_t])/np.mean(num_total_relevant_edges_list[top_k_t]))*100, 1)) + "%\n")
            #same % w.r.t. total number of relevant edges above but without rounding
            print("% w.r.t. total number of relevant edges: " + str((np.mean(num_relevant_edge_in_protein_list[top_k_t])/np.mean(num_total_relevant_edges_list[top_k_t]))*100) + "%\n")
            f.write("Avg number of relevant edges in ligand: " +  str(round(np.mean(num_relevant_edge_in_ligand_list[top_k_t]), 3)) + "\n")
            f.write("% w.r.t. total number of relevant edges: " + str(round((np.mean(num_relevant_edge_in_ligand_list[top_k_t])/np.mean(num_total_relevant_edges_list[top_k_t]))*100, 1)) + "%\n")
            #same % w.r.t. total number of relevant edges above but without rounding
            print("% w.r.t. total number of relevant edges: " + str((np.mean(num_relevant_edge_in_ligand_list[top_k_t])/np.mean(num_total_relevant_edges_list[top_k_t]))*100) + "%\n")
            f.write("Avg number of relevant edges in interaction: " + str(round(np.mean(num_relevant_edge_in_between_list[top_k_t]), 3)) + "\n")
            f.write("% w.r.t. total number of relevant edges: " + str(round((np.mean(num_relevant_edge_in_between_list[top_k_t])/np.mean(num_total_relevant_edges_list[top_k_t]))*100, 1)) + "%\n\n")
            # same % w.r.t. total number of relevant edges above but without rounding
            print("% w.r.t. total number of relevant edges: " + str((np.mean(num_relevant_edge_in_between_list[top_k_t])/np.mean(num_total_relevant_edges_list[top_k_t]))*100) + "%\n\n")




import os
from tqdm import tqdm
import json
import yaml

import torch
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader

import numpy as np
from sklearn.preprocessing import RobustScaler, MinMaxScaler

import json
import networkx as nx
import pandas as pd

from src.utils import create_edge_index, PLIDataset, set_all_seeds, GCN, GraphSAGE, GAT, GIN, GINE, GC_GNN, save_model 


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Working on device: ", device)

if __name__ == "__main__":
    with open("parameters.yml") as paramFile:  
        args = yaml.load(paramFile, Loader=yaml.FullLoader)

    DATA_PATH = args["trainer"]["DATA_PATH"]

    CLEAN_DATA = args["trainer"]["CLEAN_DATA"]
    MIN_AFFINITY = args["trainer"]["MIN_AFFINITY"]
    MAX_AFFINITY = args["trainer"]["MAX_AFFINITY"]
    NUM_CLASSES = 1 #set it up to 1 since we are facing a regression problem

    MEAN_LOWER_BOUND = args["trainer"]["MEAN_LOWER_BOUND"]
    MEAN_UPPER_BOUND = args["trainer"]["MEAN_UPPER_BOUND"]
    LOW_BOUND = args["trainer"]["LOW_BOUND"]
    HIGH_BOUND = args["trainer"]["HIGH_BOUND"]

    MODEL_NAME = args["trainer"]["GNN_MODEL"]

    GNN = GCN if MODEL_NAME == "GCN" else GraphSAGE if MODEL_NAME == "GraphSAGE" else GAT if MODEL_NAME == "GAT" else GIN if MODEL_NAME == "GIN" else GINE if MODEL_NAME == "GINE" else GC_GNN if MODEL_NAME == "GC_GNN" else None
    
    SAVE_BEST_MODEL = args["trainer"]["SAVE_BEST_MODEL"]
    MODEL_SAVE_FOLDER = args["trainer"]["MODEL_SAVE_FOLDER"]

    EDGE_WEIGHT = args["trainer"]["EDGE_WEIGHT"]
    SCALING = args["trainer"]["SCALING"]

    SEED = args["trainer"]["SEED"]
    HIDDEN_CHANNELS = args["trainer"]["HIDDEN_CHANNELS"]
    EPOCHS = args["trainer"]["EPOCHS"]
    NODE_FEATURES = args["trainer"]["NODE_FEATURES"] #if False, use dummy features (1)
    BATCH_SIZE = args["trainer"]["BATCH_SIZE"]
    LEARNING_RATE = float(args["trainer"]["LEARNING_RATE"])
    WEIGHT_DECAY = float(args["trainer"]["WEIGHT_DECAY"])
    
    set_all_seeds(SEED)

    interaction_affinities = None

    with open(DATA_PATH + '/interaction_affinities.json', 'r') as fp:
        interaction_affinities = json.load(fp)


    affinities_df = pd.DataFrame.from_dict(interaction_affinities, orient='index', columns=['affinity'])

    if CLEAN_DATA == True:
        affinities_df = affinities_df[affinities_df['affinity'] >= MIN_AFFINITY]
        affinities_df = affinities_df[affinities_df['affinity'] <= MAX_AFFINITY]

    vals_cleaned = list(affinities_df['affinity'])
    mean_interaction_affinity_no_outliers = np.mean(vals_cleaned)


    affinities_df = affinities_df.sort_values(by = "affinity", ascending=True)

    interaction_affinities = affinities_df.to_dict(orient='index')

    descriptors_interaction_dict = None
    num_node_features = 0
    if NODE_FEATURES:
        descriptors_interaction_dict = {}
        descriptors_interaction_dict["CA"] = [1, 0, 0, 0, 0, 0, 0, 0]
        descriptors_interaction_dict["NZ"] = [0, 1, 0, 0, 0, 0, 0, 0]
        descriptors_interaction_dict["N"] = [0, 0, 1, 0, 0, 0, 0, 0]
        descriptors_interaction_dict["OG"] = [0, 0, 0, 1, 0, 0, 0, 0]
        descriptors_interaction_dict["O"] = [0, 0, 0, 0, 1, 0, 0, 0]
        descriptors_interaction_dict["CZ"] = [0, 0, 0, 0, 0, 1, 0, 0]
        descriptors_interaction_dict["OD1"] = [0, 0, 0, 0, 0, 0, 1, 0]
        descriptors_interaction_dict["ZN"] = [0, 0, 0, 0, 0, 0, 0, 1]
        num_node_features = len(descriptors_interaction_dict["CA"])


    def generate_pli_dataset_dict(data_path):

        directory = os.fsencode(data_path)

        dataset_dict = {}
        dirs = os.listdir(directory)
        for file in tqdm(dirs):
            interaction_name = os.fsdecode(file)

            if interaction_name in interaction_affinities:
                if os.path.isdir(data_path + interaction_name):
                    dataset_dict[interaction_name] = {}
                    G = None
                    with open(data_path + interaction_name + "/" + interaction_name + "_interaction_graph.json", 'r') as f:
                        data = json.load(f)
                        G = nx.Graph()

                        for node in data['nodes']:
                            G.add_node(node["id"], atom_type=node["attype"], origin=node["pl"]) 

                        for edge in data['edges']:
                            if edge["id1"] != None and edge["id2"] != None:
                                G.add_edge(edge["id1"], edge["id2"], weight= float(edge["length"]))
                                

                        for node in data['nodes']:
                            nx.set_node_attributes(G, {node["id"]: node["attype"]}, "atom_type")
                            nx.set_node_attributes(G, {node["id"]: node["pl"]}, "origin")

                        
                        
                    dataset_dict[interaction_name]["networkx_graph"] = G
                    edge_index, edge_weight = create_edge_index(G, weighted=True)

                    dataset_dict[interaction_name]["edge_index"] = edge_index
                    dataset_dict[interaction_name]["edge_weight"] = edge_weight
                    

                    num_nodes = G.number_of_nodes()
                   
                    if not NODE_FEATURES:
                        dataset_dict[interaction_name]["x"] = torch.full((num_nodes, 1), 1.0, dtype=torch.float) #dummy feature
                    else:
                        dataset_dict[interaction_name]["x"] = torch.zeros((num_nodes, num_node_features), dtype=torch.float)
                        for node in G.nodes:
                            
                            dataset_dict[interaction_name]["x"][node] = torch.tensor(descriptors_interaction_dict[G.nodes[node]["atom_type"]], dtype=torch.float)
                        

                    ## gather label
                    dataset_dict[interaction_name]["y"] = torch.FloatTensor([interaction_affinities[interaction_name]["affinity"]])

        
        return dataset_dict
            
    pli_dataset_dict = generate_pli_dataset_dict(DATA_PATH + "/dataset/")

    # ### create torch dataset


    if SCALING:
        first_level = [pli_dataset_dict[key]["edge_weight"] for key in pli_dataset_dict]
        second_level = [item for sublist in first_level for item in sublist]
        if MODEL_NAME == "GCN":
            transformer = MinMaxScaler().fit(np.array(second_level).reshape(-1, 1))
        else:
            transformer = RobustScaler().fit(np.array(second_level).reshape(-1, 1))
        for key in tqdm(pli_dataset_dict):
            scaled_weights = transformer.transform(np.array(pli_dataset_dict[key]["edge_weight"]).reshape(-1, 1))
            scaled_weights = [x[0] for x in scaled_weights]
            pli_dataset_dict[key]["edge_weight"] = torch.FloatTensor(scaled_weights)
            
    data_list = []
    for interaction_name in tqdm(pli_dataset_dict):
        edge_weight_sample = None
        if EDGE_WEIGHT:
            edge_weight_sample = pli_dataset_dict[interaction_name]["edge_weight"]
        data_list.append(Data(x = pli_dataset_dict[interaction_name]["x"], edge_index = pli_dataset_dict[interaction_name]["edge_index"], edge_weight = edge_weight_sample, y = pli_dataset_dict[interaction_name]["y"], networkx_graph = pli_dataset_dict[interaction_name]["networkx_graph"], interaction_name = interaction_name))


    dataset = PLIDataset(".", data_list = data_list)


    train_interactions = []
    val_interactions = []
    core_set_interactions = []
    hold_out_interactions = []

    with open(DATA_PATH + "pdb_ids/training_set.csv", 'r') as f:
        train_interactions = f.readlines()

    train_interactions = [interaction.strip() for interaction in train_interactions]

    with open(DATA_PATH + "pdb_ids/validation_set.csv", 'r') as f:
        val_interactions = f.readlines()

    val_interactions = [interaction.strip() for interaction in val_interactions]

    with open(DATA_PATH + "pdb_ids/core_set.csv", 'r') as f:
        core_set_interactions = f.readlines()

    core_set_interactions = [interaction.strip() for interaction in core_set_interactions]

    with open(DATA_PATH + "pdb_ids/hold_out_set.csv", 'r') as f:
        hold_out_interactions = f.readlines()

    hold_out_interactions = [interaction.strip() for interaction in hold_out_interactions]

    train_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in train_interactions]
    val_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in val_interactions]
    core_set_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in core_set_interactions]
    hold_out_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in hold_out_interactions]

    rng = np.random.default_rng(seed = SEED)
    rng.shuffle(train_data)
    rng.shuffle(val_data)
    rng.shuffle(core_set_data)
    rng.shuffle(hold_out_data)


    print("Number of samples after outlier removal: ", len(dataset))
    print("Number of training samples: ", len(train_data))
    print("Number of validation samples: ", len(val_data))
    print("Number of core set samples: ", len(core_set_data))
    print("Number of core set low affinity samples: ", len([sample for sample in core_set_data if sample.y < LOW_BOUND]))
    print("Number of core set medium affinity samples: ", len([sample for sample in core_set_data if sample.y >= MEAN_LOWER_BOUND and sample.y <= MEAN_UPPER_BOUND]))
    print("Number of core set high affinity samples: ", len([sample for sample in core_set_data if sample.y > HIGH_BOUND]))
    print("Number of hold out samples: ", len(hold_out_data))
    print("Number of hold out low affinity samples: ", len([sample for sample in hold_out_data if sample.y < LOW_BOUND]))
    print("Number of hold out medium affinity samples: ", len([sample for sample in hold_out_data if sample.y >= MEAN_LOWER_BOUND and sample.y <= MEAN_UPPER_BOUND]))
    print("Number of hold out high affinity samples: ", len([sample for sample in hold_out_data if sample.y > HIGH_BOUND]))

    core_set_hold_out_interactions = core_set_interactions + hold_out_interactions
    core_set_hold_out_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in core_set_hold_out_interactions]

    print("Number of test (core + hold out) samples: ", len(core_set_hold_out_data))
    print("Number of test low affinity samples: ", len([sample for sample in core_set_hold_out_data if sample.y < LOW_BOUND]))
    print("Number of test medium affinity samples: ", len([sample for sample in core_set_hold_out_data if sample.y >= MEAN_LOWER_BOUND and sample.y <= MEAN_UPPER_BOUND]))
    print("Number of test high affinity samples: ", len([sample for sample in core_set_hold_out_data if sample.y > HIGH_BOUND]))


    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)
    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)
    core_set_loader = DataLoader(core_set_data, batch_size=BATCH_SIZE)
    hold_out_loader = DataLoader(hold_out_data, batch_size=BATCH_SIZE)
    

    # ### Train the network

    model = GNN(node_features_dim = dataset[0].x.shape[1], num_classes = NUM_CLASSES, hidden_channels=HIDDEN_CHANNELS).to(device)

    lr = LEARNING_RATE

    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)

    criterion = torch.nn.MSELoss()
        
    epochs = EPOCHS


    def train():
        model.train()

        for data in train_loader:  # Iterate in batches over the training dataset.
            data = data.to(device)
            out = model(data.x, data.edge_index, data.batch, edge_weight = data.edge_weight)  # Perform a single forward pass.
            
            loss = torch.sqrt(criterion(torch.squeeze(out), data.y))  # Compute the loss.
        
            loss.backward()  # Derive gradients.
            optimizer.step()  # Update parameters based on gradients.
            optimizer.zero_grad()  # Clear gradients.

    def test(loader):
        model.eval()

        sum_loss = 0
        for data in loader:  # Iterate in batches over the training/test dataset.
            data = data.to(device)
            
            out = model(data.x, data.edge_index, data.batch, edge_weight = data.edge_weight)  
            
            if  data.y.shape[0] == 1:
                loss = torch.sqrt(criterion(torch.squeeze(out, 1), data.y))
            else:
                loss = torch.sqrt(criterion(torch.squeeze(out), data.y)) * data.y.shape[0]
            sum_loss += loss.item()
            
        return sum_loss / len(loader.dataset) 



    best_epoch = 0

    best_val_loss = 100000
    for epoch in range(epochs):
        train()
        train_rmse = test(train_loader)
        val_rmse = test(val_loader)
        if val_rmse < best_val_loss:
            best_val_loss = val_rmse
            best_epoch = epoch
            if SAVE_BEST_MODEL:
                save_model(model, MODEL_SAVE_FOLDER, model_name = MODEL_NAME + "_best")
            
        print(f'Epoch: {epoch:03d}, Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}')

    core_set_rmse = test(core_set_loader)    
    hold_out_set_rmse = test(hold_out_loader)

    if not SAVE_BEST_MODEL:
        print(f'Core set 2016 RMSE with latest model: {core_set_rmse:.4f}')
        print(f'Hold out set 2019 RMSE with latest model: {hold_out_set_rmse:.4f}')
        save_model(model, MODEL_SAVE_FOLDER, model_name = MODEL_NAME + "_latest", timestamp=True)

    print(f'Best model at epoch: {best_epoch:03d}')
    print("Best val loss: ", best_val_loss)


    if SAVE_BEST_MODEL:
        model = GNN(node_features_dim = dataset[0].x.shape[1], num_classes = NUM_CLASSES, hidden_channels=HIDDEN_CHANNELS).to(device)
        model.load_state_dict(torch.load("models/model_" + MODEL_NAME + "_best.ckpt"))
        model.to(device)

        core_set_rmse = test(core_set_loader)    
        print(f'Core set 2016 RMSE with best model: {core_set_rmse:.4f}')

        hold_out_set_rmse = test(hold_out_loader)    
        print(f'Hold out set 2019 RMSE with best model: {hold_out_set_rmse:.4f}')
 
        os.rename("models/model_" + MODEL_NAME + "_best.ckpt", "models/model_" + MODEL_NAME + "_best_" + str(best_epoch) + ".ckpt")



import os
from tqdm import tqdm
import json
import yaml

import torch
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader

import numpy as np
from sklearn.preprocessing import RobustScaler, MinMaxScaler


import json
import networkx as nx
import pandas as pd

from src.utils import create_edge_index, PLIDataset, set_all_seeds, GCN, GraphSAGE, GAT, GIN, GINE, GC_GNN
from src.edgeshaper import Edgeshaper

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Working on device: ", device)

with open("parameters.yml") as paramFile:  
        args = yaml.load(paramFile, Loader=yaml.FullLoader)

DATA_PATH = args["explainer"]["DATA_PATH"]
SAVE_FOLDER = args["explainer"]["SAVE_FOLDER"]

CLEAN_DATA = args["explainer"]["CLEAN_DATA"]
MIN_AFFINITY = args["explainer"]["MIN_AFFINITY"]
MAX_AFFINITY = args["explainer"]["MAX_AFFINITY"]
NUM_CLASSES = 1 #set it up to 1 since we are facing a regression problem

MEAN_LOWER_BOUND = args["explainer"]["MEAN_LOWER_BOUND"]
MEAN_UPPER_BOUND = args["explainer"]["MEAN_UPPER_BOUND"]
LOW_BOUND = args["explainer"]["LOW_BOUND"]
HIGH_BOUND = args["explainer"]["HIGH_BOUND"]

MODEL_NAME = args["explainer"]["GNN_MODEL"]
GNN = GCN if MODEL_NAME == "GCN" else GraphSAGE if MODEL_NAME == "GraphSAGE" else GAT if MODEL_NAME == "GAT" else GIN if MODEL_NAME == "GIN" else GINE if MODEL_NAME == "GINE" else GC_GNN if MODEL_NAME == "GC_GNN" else None
MODEL_PATH = args["explainer"]["MODEL_PATH"]

print("Using model: ", MODEL_NAME)

EDGE_WEIGHT = args["explainer"]["EDGE_WEIGHT"]
SCALING = args["explainer"]["SCALING"]
BATCH_SIZE = args["explainer"]["BATCH_SIZE"]
LEARNING_RATE = float(args["explainer"]["LEARNING_RATE"])
WEIGHT_DECAY = float(args["explainer"]["WEIGHT_DECAY"])

SEED = args["explainer"]["SEED"]
HIDDEN_CHANNELS = args["explainer"]["HIDDEN_CHANNELS"]
EPOCHS = args["explainer"]["EPOCHS"]
NODE_FEATURES = args["explainer"]["NODE_FEATURES"] #if False, use dummy features (1)
AFFINITY_SET = args["explainer"]["AFFINITY_SET"] 

assert(AFFINITY_SET == "low" or AFFINITY_SET == "high" or AFFINITY_SET == "medium")

print("Explaining affinity set: ", AFFINITY_SET)

SAMPLES_TO_EXPLAIN = args["explainer"]["SAMPLES_TO_EXPLAIN"]

set_all_seeds(SEED)


interaction_affinities = None

with open(DATA_PATH + '/interaction_affinities.json', 'r') as fp:
    interaction_affinities = json.load(fp)



affinities_df = pd.DataFrame.from_dict(interaction_affinities, orient='index', columns=['affinity'])

if CLEAN_DATA == True:
    affinities_df = affinities_df[affinities_df['affinity'] >= MIN_AFFINITY]
    affinities_df = affinities_df[affinities_df['affinity'] <= MAX_AFFINITY]

vals_cleaned = list(affinities_df['affinity'])
mean_interaction_affinity_no_outliers = np.mean(vals_cleaned)

affinities_df = affinities_df.sort_values(by = "affinity", ascending=True)

interaction_affinities = affinities_df.to_dict(orient='index')

descriptors_interaction_dict = None
num_node_features = 0
if NODE_FEATURES:
    descriptors_interaction_dict = {}
    descriptors_interaction_dict["CA"] = [1, 0, 0, 0, 0, 0, 0, 0]
    descriptors_interaction_dict["NZ"] = [0, 1, 0, 0, 0, 0, 0, 0]
    descriptors_interaction_dict["N"] = [0, 0, 1, 0, 0, 0, 0, 0]
    descriptors_interaction_dict["OG"] = [0, 0, 0, 1, 0, 0, 0, 0]
    descriptors_interaction_dict["O"] = [0, 0, 0, 0, 1, 0, 0, 0]
    descriptors_interaction_dict["CZ"] = [0, 0, 0, 0, 0, 1, 0, 0]
    descriptors_interaction_dict["OD1"] = [0, 0, 0, 0, 0, 0, 1, 0]
    descriptors_interaction_dict["ZN"] = [0, 0, 0, 0, 0, 0, 0, 1]
    num_node_features = len(descriptors_interaction_dict["CA"])

def generate_pli_dataset_dict(data_path):

        directory = os.fsencode(data_path)

        dataset_dict = {}
        dirs = os.listdir(directory)
        dirs = sorted(dirs, key = str)
        
        for file in tqdm(dirs):
            interaction_name = os.fsdecode(file)

            if interaction_name in interaction_affinities:
                if os.path.isdir(data_path + interaction_name):
                    dataset_dict[interaction_name] = {}
                    G = None
                    with open(data_path + interaction_name + "/" + interaction_name + "_interaction_graph.json", 'r') as f:
                        data = json.load(f)
                        G = nx.Graph()

                        for node in data['nodes']:
                            G.add_node(node["id"], atom_type=node["attype"], origin=node["pl"]) 

                        for edge in data['edges']:
                            if edge["id1"] != None and edge["id2"] != None:
                                G.add_edge(edge["id1"], edge["id2"], weight= float(edge["length"]))
                                

                        for node in data['nodes']:
                            nx.set_node_attributes(G, {node["id"]: node["attype"]}, "atom_type")
                            nx.set_node_attributes(G, {node["id"]: node["pl"]}, "origin")

                        
                        
                    dataset_dict[interaction_name]["networkx_graph"] = G
                    edge_index, edge_weight = create_edge_index(G, weighted=True)

                    dataset_dict[interaction_name]["edge_index"] = edge_index
                    dataset_dict[interaction_name]["edge_weight"] = edge_weight
                    

                    num_nodes = G.number_of_nodes()
                   
                    if not NODE_FEATURES:
                        dataset_dict[interaction_name]["x"] = torch.full((num_nodes, 1), 1.0, dtype=torch.float) #dummy feature
                    else:
                        dataset_dict[interaction_name]["x"] = torch.zeros((num_nodes, num_node_features), dtype=torch.float)
                        for node in G.nodes:
                            
                            dataset_dict[interaction_name]["x"][node] = torch.tensor(descriptors_interaction_dict[G.nodes[node]["atom_type"]], dtype=torch.float)
                        

                    ## gather label
                    dataset_dict[interaction_name]["y"] = torch.FloatTensor([interaction_affinities[interaction_name]["affinity"]])

        
        return dataset_dict
        
pli_dataset_dict = generate_pli_dataset_dict(DATA_PATH + "/dataset/")


if SCALING:
    first_level = [pli_dataset_dict[key]["edge_weight"] for key in pli_dataset_dict]
    second_level = [item for sublist in first_level for item in sublist]
    if MODEL_NAME == "GCN":
        transformer = MinMaxScaler().fit(np.array(second_level).reshape(-1, 1))
    else:
        transformer = RobustScaler().fit(np.array(second_level).reshape(-1, 1))
    for key in tqdm(pli_dataset_dict):
        scaled_weights = transformer.transform(np.array(pli_dataset_dict[key]["edge_weight"]).reshape(-1, 1))
        scaled_weights = [x[0] for x in scaled_weights]
        pli_dataset_dict[key]["edge_weight"] = torch.FloatTensor(scaled_weights)
        
data_list = []
for interaction_name in tqdm(pli_dataset_dict):
    edge_weight_sample = None
    if EDGE_WEIGHT:
        edge_weight_sample = pli_dataset_dict[interaction_name]["edge_weight"]
    data_list.append(Data(x = pli_dataset_dict[interaction_name]["x"], edge_index = pli_dataset_dict[interaction_name]["edge_index"], edge_weight = edge_weight_sample, y = pli_dataset_dict[interaction_name]["y"], networkx_graph = pli_dataset_dict[interaction_name]["networkx_graph"], interaction_name = interaction_name))


dataset = PLIDataset(".", data_list = data_list)



train_interactions = []
val_interactions = []
core_set_interactions = []
hold_out_interactions = []

with open(DATA_PATH + "pdb_ids/training_set.csv", 'r') as f:
    train_interactions = f.readlines()

train_interactions = [interaction.strip() for interaction in train_interactions]

with open(DATA_PATH + "pdb_ids/validation_set.csv", 'r') as f:
    val_interactions = f.readlines()

val_interactions = [interaction.strip() for interaction in val_interactions]

with open(DATA_PATH + "pdb_ids/core_set.csv", 'r') as f:
    core_set_interactions = f.readlines()

core_set_interactions = [interaction.strip() for interaction in core_set_interactions]

with open(DATA_PATH + "pdb_ids/hold_out_set.csv", 'r') as f:
    hold_out_interactions = f.readlines()

hold_out_interactions = [interaction.strip() for interaction in hold_out_interactions]


train_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in train_interactions]
val_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in val_interactions]
core_set_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in core_set_interactions]
hold_out_data = [dataset[i] for i in range(len(dataset)) if dataset[i].interaction_name in hold_out_interactions]

#alternative here is to comment the shuffle out and use shuffling in the dataloader
rng = np.random.default_rng(seed = SEED)
rng.shuffle(train_data)
rng.shuffle(val_data)
rng.shuffle(core_set_data)
rng.shuffle(hold_out_data)

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)
val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)
core_set_loader = DataLoader(core_set_data, batch_size=BATCH_SIZE)
hold_out_loader = DataLoader(hold_out_data, batch_size=BATCH_SIZE)


model = GNN(node_features_dim = dataset[0].x.shape[1], num_classes = NUM_CLASSES, hidden_channels=HIDDEN_CHANNELS).to(device)

lr = LEARNING_RATE

optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)
criterion = torch.nn.MSELoss()
epochs = EPOCHS

def train():
        model.train()

        for data in train_loader:  
            data = data.to(device)
            out = model(data.x, data.edge_index, data.batch, edge_weight = data.edge_weight)  
            
            loss = torch.sqrt(criterion(torch.squeeze(out), data.y))  
        
            loss.backward()  
            optimizer.step()  
            optimizer.zero_grad()  

def test(loader):
    model.eval()

    sum_loss = 0
    for data in loader: 
        data = data.to(device)
        
        out = model(data.x, data.edge_index, data.batch, edge_weight = data.edge_weight)  
        
        if  data.y.shape[0] == 1:
            loss = torch.sqrt(criterion(torch.squeeze(out, 1), data.y))
        else:
            loss = torch.sqrt(criterion(torch.squeeze(out), data.y)) * data.y.shape[0]
        sum_loss += loss.item()
        
    return sum_loss / len(loader.dataset) 


#load model
model.load_state_dict(torch.load(MODEL_PATH)) 
model.to(device)
model

core_set_rmse = test(core_set_loader)    
print(f'Core set 2016 RMSE with loaded model: {core_set_rmse:.4f}')

hold_out_set_rmse = test(hold_out_loader)    
print(f'Hold out set 2019 RMSE with loaded model: {hold_out_set_rmse:.4f}')

#explanation phase

num_all_test_interactions = len(hold_out_data)
rng = np.random.default_rng(seed=SEED)
all_test_interaction_indices = np.array(range(num_all_test_interactions))
rng.shuffle(all_test_interaction_indices)

num_edge_in_protein_list = []
num_edge_in_ligand_list = []
num_edge_in_between_list = []
num_total_relevant_edges_list = []

num_edge_in_protein_abs_list = []
num_edge_in_ligand_abs_list = []
num_edge_in_between_abs_list = []
num_total_relevant_edges_abs_list = []

num_total_edge_in_protein_list = []
num_total_edge_in_ligand_list = []
num_total_edge_in_between_list = []
num_total_edges_in_graph_list = []

num_pert_pos_edges_in_protein_list = []
num_pert_pos_edges_in_ligand_list = []
num_pert_pos_edges_in_between_list = []
num_total_pert_pos_edges_list = []

num_min_top_k_edges_in_protein_list = []
num_min_top_k_edges_in_ligand_list = []
num_min_top_k_edges_in_between_list = []
num_total_min_top_k_edges_list = []

fidelity_score_list = []
infidelity_score_list = []
trustworthiness_score_list = []

test_interaction_indices = []
num_test_interactions = SAMPLES_TO_EXPLAIN
test_interaction_names = []
test_interactions_affinities = []
test_interaction_names_affinities_dict = {}


for test_interaction_index in all_test_interaction_indices:
    model.eval()
    test_interaction = hold_out_data[test_interaction_index]

    edge_weight_to_pass = None
    if EDGE_WEIGHT:
        edge_weight_to_pass = test_interaction.edge_weight.to(device)

    batch = torch.zeros(test_interaction.x.shape[0], dtype=int, device=test_interaction.x.device)
    
    test_affinity_value = test_interaction.y.item()
    if AFFINITY_SET == "medium":
        if test_affinity_value < MEAN_LOWER_BOUND or test_affinity_value > MEAN_UPPER_BOUND:
            continue
    elif AFFINITY_SET == "low":
        if test_affinity_value >= LOW_BOUND:
            continue
    else:
        if test_affinity_value <= HIGH_BOUND:
            continue

    
    out = model(test_interaction.x.to(device), test_interaction.edge_index.to(device), batch=batch.to(device), edge_weight=edge_weight_to_pass)
    pred = out.item()
    if AFFINITY_SET == "low":
        if pred >= MEAN_LOWER_BOUND:
            continue
    elif AFFINITY_SET == "high":
        if pred <= MEAN_UPPER_BOUND:
            continue
    else:
        if pred < MEAN_LOWER_BOUND or pred > MEAN_UPPER_BOUND:
            continue
        
    test_interaction_indices.append(test_interaction_index)
    test_interaction_names.append(test_interaction.interaction_name)
    test_interactions_affinities.append(test_affinity_value)
    test_interaction_names_affinities_dict[test_interaction.interaction_name] = test_affinity_value
    
    if len(test_interaction_indices) == num_test_interactions:
        break


TARGET_CLASS = None

    
for index in tqdm(test_interaction_indices):
    model.eval()
    test_interaction = hold_out_data[index]
    print("\nInteraction: " + test_interaction.interaction_name)

    edge_weight_to_pass = None
    if EDGE_WEIGHT:
        edge_weight_to_pass = test_interaction.edge_weight.to(device)

    batch = torch.zeros(test_interaction.x.shape[0], dtype=int, device=test_interaction.x.device)
    
    
    out = model(test_interaction.x.to(device), test_interaction.edge_index.to(device), batch=batch.to(device), edge_weight=edge_weight_to_pass) #test_interaction.edge_weight.to(device)
    
    
    #explainability

    edgeshaper_explainer = Edgeshaper(model, test_interaction.x, test_interaction.edge_index, edge_weight = test_interaction.edge_weight, device = device)

    phi_edges = edgeshaper_explainer.explain(M = 100, target_class = TARGET_CLASS, deviation = None, seed = SEED) #deviation = 1e-3

    #plotting
    num_bonds = test_interaction.networkx_graph.number_of_edges()
    rdkit_bonds_phi = [0]*num_bonds
    rdkit_bonds = {}

    bonds = dict(test_interaction.networkx_graph.edges())
    bonds = list(bonds.keys())

    for i in range(num_bonds):
        init_atom = bonds[i][0]
        end_atom = bonds[i][1]
        
        rdkit_bonds[(init_atom, end_atom)] = i

    for i in range(len(phi_edges)):
        phi_value = phi_edges[i]
        init_atom = test_interaction.edge_index[0][i].item()
        end_atom = test_interaction.edge_index[1][i].item()
        
        if (init_atom, end_atom) in rdkit_bonds:
            bond_index = rdkit_bonds[(init_atom, end_atom)]
            rdkit_bonds_phi[bond_index] += phi_value
        if (end_atom, init_atom) in rdkit_bonds:
            bond_index = rdkit_bonds[(end_atom, init_atom)]
            rdkit_bonds_phi[bond_index] += phi_value

    
    

    SAVE_PATH = SAVE_FOLDER + "/" + MODEL_NAME + "/" + AFFINITY_SET + " affinity" + "/" + test_interaction.interaction_name + "/"
    
    
    if not os.path.exists(SAVE_PATH):
        os.makedirs(SAVE_PATH)


    with open(SAVE_PATH + test_interaction.interaction_name + "_statistics.txt", "w+") as f:
        f.write("Interaction name: " + test_interaction.interaction_name + "\n\n")
        f.write("Affinity: " + str(test_interaction.y.item()) + "\n")
        f.write("Predicted value: " + str(out.item()) + "\n\n")


        f.write("Shapley values for edges: \n\n")
        for i in range(len(phi_edges)):
            f.write("(" + str(test_interaction.edge_index[0][i].item()) + "," + str(test_interaction.edge_index[1][i].item()) + "): " + str(phi_edges[i]) + "\n")